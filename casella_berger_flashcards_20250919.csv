Front,Back
"What is a random variable?","A random variable is a function that assigns a real number to each outcome in a sample space. It's essentially a rule that converts the results of a random experiment into numerical values that we can work with mathematically."
"What's the difference between discrete and continuous random variables?","A discrete random variable can only take on countably many values (like integers), often resulting from counting. A continuous random variable can take on any value in an interval, often resulting from measurement. The key difference is whether the variable has ""gaps"" between possible values."
"What is a probability mass function (pmf)?","A pmf describes the probability distribution of a discrete random variable. It gives the probability that the variable equals each specific value. The pmf must be non-negative and all probabilities must sum to 1."
"What is a probability density function (pdf)?","A pdf describes the probability distribution of a continuous random variable. Unlike a pmf, the pdf value at a point is not a probability - instead, probabilities are found by integrating the pdf over intervals. The pdf must be non-negative and integrate to 1."
"What is a cumulative distribution function (cdf)?","The cdf gives the probability that a random variable is less than or equal to a given value. It works for both discrete and continuous variables, always increases from 0 to 1, and is right-continuous. It completely characterizes the distribution."
"What are the key properties of any cumulative distribution function?","Every cdf must satisfy: (1) values between 0 and 1, (2) non-decreasing, (3) right-continuous, (4) approaches 0 as x approaches negative infinity, and (5) approaches 1 as x approaches positive infinity."
"What does conditional probability tell us?","Conditional probability P(A|B) gives the probability of event A occurring given that we know event B has occurred. It updates our knowledge based on new information and is calculated as P(A∩B)/P(B) when P(B) > 0."
"When are two events independent?","Two events are independent if knowing that one occurred doesn't change the probability of the other occurring. Mathematically, A and B are independent if P(A|B) = P(A), or equivalently, P(A∩B) = P(A)P(B)."
"What does it mean for random variables to be identically distributed?","Two random variables are identically distributed if they have the same probability distribution - meaning they have the same cdf (or pmf/pdf). Note that identically distributed variables are not necessarily equal; they just follow the same probability laws."
"How do we find the distribution of a function of a random variable?","For Y = g(X), we can find the distribution of Y using the transformation method. For continuous variables, we often use the inverse transformation and the Jacobian. For discrete variables, we find where each y-value comes from in the original distribution."
"What is the expected value of a random variable?","The expected value (or mean) is the long-run average value of a random variable. For discrete variables, it's the sum of each value times its probability. For continuous variables, it's the integral of x times the pdf. It represents the ""center"" of the distribution."
"What does variance measure?","Variance measures how spread out a distribution is around its mean. It's the expected value of the squared deviations from the mean: Var(X) = E[(X - μ)²]. Larger variance means more spread; variance of zero means the variable is constant."
"What is standard deviation and why is it useful?","Standard deviation is the square root of variance. It measures spread in the same units as the original variable, making it easier to interpret than variance. A small standard deviation means values cluster near the mean; a large one means they're more dispersed."
"What are moments of a distribution?","Moments are expected values of powers of a random variable. The first moment is the mean, the second central moment is the variance. Higher moments describe the shape of the distribution - the third moment relates to skewness, the fourth to kurtosis."
"What is a moment generating function and why is it useful?","The moment generating function (mgf) is M(t) = E[e^(tX)]. If it exists in a neighborhood of zero, it uniquely determines the distribution and can be used to find all moments by taking derivatives at t=0. It's also useful for proving convergence results."
"What are the key properties of moment generating functions?","If the mgf exists: (1) M(0) = 1 always, (2) the nth derivative at t=0 gives the nth moment, (3) M(t) uniquely determines the distribution, (4) for Y = aX + b, M_Y(t) = e^(bt)M_X(at), and (5) convergence of mgfs implies convergence of distributions."
"How do expectations behave with linear combinations?","Expectation is linear: E[aX + bY + c] = aE[X] + bE[Y] + c for any constants a, b, c. This holds regardless of whether X and Y are independent. However, variance is not linear unless variables are independent."
"How does variance behave with linear combinations?","For Var(aX + b), we get a²Var(X) - the constant b disappears and a gets squared. For sums, Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y). If X and Y are independent, the covariance term is zero."
"What's the relationship between independence and correlation?","Independence implies zero correlation (uncorrelated), but zero correlation doesn't imply independence. Independence is a stronger condition - it means the variables don't depend on each other in any way, while zero correlation only means they're not linearly related."
"What is the law of total probability?","If events B₁, B₂, ..., Bₙ partition the sample space, then P(A) = Σ P(A|Bᵢ)P(Bᵢ). This lets us calculate the probability of A by conditioning on different scenarios and weighing each by how likely that scenario is."
"What is Bayes' theorem and when is it used?","Bayes' theorem gives P(B|A) = P(A|B)P(B)/P(A). It's used to ""reverse"" conditional probabilities - if we know P(A|B) but want P(B|A). It's fundamental to updating beliefs based on new evidence."
"What's the key difference between probability and density for continuous variables?","For continuous variables, the pdf value at a point is not a probability - individual points have probability zero. Instead, probabilities are areas under the curve (integrals). The pdf tells us about the relative likelihood or ""concentration"" of values in different regions."
"What's the general approach for transforming continuous random variables?","For Y = g(X) where g is monotonic: (1) find the inverse function X = g⁻¹(Y), (2) differentiate to get dx/dy, (3) substitute into f_Y(y) = f_X(g⁻¹(y))|dx/dy|. For non-monotonic functions, split into monotonic pieces and add the contributions."
"What are some useful expectation formulas to remember?","E[aX + b] = aE[X] + b, E[X²] = Var(X) + [E(X)]², E[XY] = E[X]E[Y] if independent, and for finding E[g(X)] you can either use the definition with the original pdf or transform to find the pdf of Y = g(X) first."
"When do moments exist?","The nth moment E[|X|ⁿ] exists if the integral (or sum) converges. Heavy-tailed distributions may not have all moments. If the nth moment exists, all lower moments also exist. The mgf existing implies all moments exist, but having all moments doesn't guarantee the mgf exists."
"When does a moment generating function uniquely determine a distribution?","If the mgf exists in some open interval containing zero, then it uniquely determines the distribution. Even if all moments exist, they might not uniquely determine the distribution unless additional conditions are met (like bounded support or Carleman's condition)."
"What are the three fundamental axioms of probability?","Kolmogorov's axioms: (1) P(A) ≥ 0 for any event A, (2) P(S) = 1 where S is the sample space, and (3) for disjoint events A₁, A₂, ..., P(∪Aᵢ) = ΣP(Aᵢ). All other probability rules follow from these axioms."
"What is a sample space?","The sample space S is the set of all possible outcomes of a random experiment. It must be exhaustive (include all possibilities) and mutually exclusive (outcomes don't overlap). Events are subsets of the sample space."
"What makes a function of a random variable also a random variable?","If X is a random variable and g is a function, then Y = g(X) is also a random variable. The key is that Y assigns a number to each outcome in the sample space through the composition: outcome → X(outcome) → g(X(outcome)) = Y(outcome)."
"What special properties do continuous random variables have?","For continuous random variables: (1) P(X = any specific value) = 0, (2) P(a ≤ X ≤ b) = P(a < X < b) = P(a ≤ X < b) = P(a < X ≤ b), (3) the cdf is continuous, and (4) probabilities are found by integrating the pdf."